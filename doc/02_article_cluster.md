# Масштабирование ClickHouse и управление миграциями

В предыдущей [статье](https://habrahabr.ru/company/smi2/blog/314558/) мы поделились своим опытом внедрения и использования СУБД ClickHouse в компании [СМИ2](https://smi2.net/). В текущей статье затронем вопросы масштабирования, когда с увеличением объема анализируемых данных и ростом нагрузки данные уже не могут храниться и обрабатываться в рамках одного физического сервера. А также поделимся инструментом миграции [DDL](https://en.wikipedia.org/wiki/Data_definition_language)-запросов.

[ClickHouse](https://clickhouse.yandex/) специально проектировался для работы в кластерах, расположенных в разных дата-центрах. Масштабируется СУБД линейно до сотен узлов. На момент написания статьи [Яндекс.Метрика](https://metrika.yandex.ru/) - это кластер из более чем 400 узлов. 

"Из коробки" ClickHouse предоставляет шардирование и репликацию, которые могут гибко настраиваться отдельно для каждой таблицы. Для обеспечения работы реплицирования требуется [Apache ZooKeeper](https://zookeeper.apache.org/) (рекомендуется использовать версию 3.4.5+). Для обеспечения более высокой надежности мы используем ZK-кластер (ансамбль) из 5 узлов. Следует выбирать нечетное число ZK-узлов, например, 3 или 5, чтобы обеспечить кворум. Также отметим, что ZK не используется в операциях SELECT, а применяется, например, в ALTER-запросах для изменений столбцов, сохраняя инструкции для каждой из реплик.   

### Шардирование

В ClickHouse шардирование позволяет записывать и хранить порции данных распределенно в кластере, и обрабатывать (читать) данные параллельно на всех узлах кластера, увеличивая throughput и уменьшая latency. Например, в запросах с GROUP BY ClickHouse выполнит агрегирование на удаленных узлах и передаст узлу-инициатору запроса промежуточные состояния агрегатных функций, где они буду доагрегированы.

Для шардирования используется специальный движок: [Distributed](https://clickhouse.yandex/reference_ru.html#Distributed), который не хранит данные, а делегирует SELECT-запросы на шардированные таблицы (таблицы, содержащие порции данных) с последующей обработкой полученных данных. Запись данных в шарды может выполняться в двух режимах: через Distributed-таблицу и необязательный ключ шардирования или непосредственно в шардированные таблицы, из которых далее будут читаться данные через Distributed-таблицу. Рассмотрим эти режимы более подробно. 
       
В простом случае данные записываются в Distributed-таблицу по ключу шардирования. В простейшем случае ключом шардирования может быть случайное число, т.е. результат вызова функции [rand()](https://clickhouse.yandex/reference_ru.html#rand). Однако, в качестве ключа шардирования рекомендуется брать значение хеш-функции от поля в таблице, которое позволит, с одной стороны, локализовать небольшие наборы данных на одном шарде, а с другой, обеспечит достаточно ровное распределение таких наборов по разным шардам в кластере. Например, идентификатор сессии (sess_id) пользователя позволит локализовать показы страниц одному пользователю на одном шарде, а с другой стороны сессии разных пользователей будут распределены равномерно по всем шардам в кластере. При условии, что значения поля sess_id будут иметь хорошее распределение. Ключ шардирования может быть нечисловой или составной, тогда можно применять втроенную хеширующую функцию [cityHash64](https://clickhouse.yandex/reference_ru.html#cityHash64). В данном режиме данные, записываемые на один из узлов кластера, по ключу шардирования будут перенаправляться на нужные шарды автоматически, правда, увеличивая трафик.
 
Более сложный способ заключается в том, чтобы снаружи ClickHouse вычислять нужный шард и выполнять запись напрямую в шардированную таблицу. Сложность здесь обусловлена тем, что нужно знать набор доступных узлов-шардов. Однако, запись становится более эффективной и механизм шардирования (определения нужного шарда) может быть более гибким.

### Репликация

ClickHouse поддерживает [репликацию](https://clickhouse.yandex/reference_ru.html#Репликация данных) данных, обеспечивая целостность данных на репликах. Для репликации данных используются специальные движки MergeTree-семейства:

* ReplicatedMergeTree
* ReplicatedCollapsingMergeTree
* ReplicatedAggregatingMergeTree
* ReplicatedSummingMergeTree

Репликация часто применяется с шардированием. Например, кластер из 6-узлов может содержать 3 шарда по 2 реплики. Следует отметить, что репликация не зависит от механизмов шардирования, и работает на уровне отдельных таблиц.
  
Запись данных может выполняться в любую из таблиц-реплик, ClickHouse выполняет автоматическую синхронизацию данных между всеми репликами.

### Инструмент миграции DDL-запросов

На момент написания статьи ClickHouse имеет ряд особенностей (ограничений) связанных с DDL-запросами. [Цитата](https://clickhouse.yandex/reference_ru.html#Репликация данных):

> Реплицируются INSERT, ALTER (см. подробности в описании запроса ALTER). Реплицируются сжатые данные, а не тексты запросов. Запросы CREATE, DROP, ATTACH, DETACH, RENAME не реплицируются - то есть, относятся к одному серверу. Запрос CREATE TABLE создаёт новую реплицируемую таблицу на том сервере, где выполняется запрос; а если на других серверах такая таблица уже есть - добавляет новую реплику. Запрос DROP TABLE удаляет реплику, расположенную на том сервере, где выполняется запрос. Запрос RENAME переименовывает таблицу на одной из реплик - то есть, реплицируемые таблицы на разных репликах могут называться по разному.

Когда количество узлов кластера становится большим, то управление кластером становится неудобным. В результате мы создали простой и достаточно функциональный инструмент для миграции DDL-запросов в ClickHouse-кластер. И работу с кластером продемонстрируем на примере.  

*бла-бла-бла про тулл Игоря...*