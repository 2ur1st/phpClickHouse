# Масштабирование ClickHouse и управление миграциями

В предыдущей [статье](https://habrahabr.ru/company/smi2/blog/314558/) мы поделились своим опытом внедрения и использования СУБД ClickHouse в компании [СМИ2](https://smi2.net/). В текущей статье затронем вопросы масштабирования, когда с увеличением объема анализируемых данных и ростом нагрузки данные уже не могут храниться и обрабатываться в рамках одного физического сервера. А также поделимся инструментом миграции [DDL](https://en.wikipedia.org/wiki/Data_definition_language)-запросов.

[ClickHouse](https://clickhouse.yandex/) специально проектировался для работы в кластерах, расположенных в разных дата-центрах. Масштабируется СУБД линейно до сотен узлов. На момент написания статьи [Яндекс.Метрика](https://metrika.yandex.ru/) - это кластер из более чем 400 узлов. 

"Из коробки" ClickHouse предоставляет шардирование и репликацию, которые могут гибко настраиваться отдельно для каждой таблицы. Для обеспечения работы реплицирования требуется [Apache ZooKeeper](https://zookeeper.apache.org/) (рекомендуется использовать версию 3.4.5+). Для обеспечения более высокой надежности мы используем ZK-кластер (ансамбль) из 5 узлов. Следует выбирать нечетное число ZK-узлов, например, 3 или 5, чтобы обеспечить кворум. Также отметим, что ZK не используется в операциях SELECT, а применяется, например, в ALTER-запросах для изменений столбцов, сохраняя инструкции для каждой из реплик.   

### Шардирование

В ClickHouse шардирование позволяет записывать и хранить порции данных распределенно в кластере, и обрабатывать (читать) данные параллельно на всех узлах кластера, увеличивая throughput и уменьшая latency. Например, в запросах с GROUP BY ClickHouse выполнит агрегирование на удаленных узлах и передаст узлу-инициатору запроса промежуточные состояния агрегатных функций, где они буду доагрегированы.

Для шардирования используется специальный движок: [Distributed](https://clickhouse.yandex/reference_ru.html#Distributed), который не хранит данные, а делегирует SELECT-запросы на шардированные таблицы (таблицы, содержащие порции данных) с последующей обработкой полученных данных. Запись данных в шарды может выполняться в двух режимах: через Distributed-таблицу и необязательный ключ шардирования или непосредственно в шардированные таблицы, из которых далее будут читаться данные через Distributed-таблицу. Рассмотрим эти режимы более подробно. 
       
В простом случае данные записываются в Distributed-таблицу по ключу шардирования. В простейшем случае ключом шардирования может быть случайное число, т.е. результат вызова функции [rand()](https://clickhouse.yandex/reference_ru.html#rand). Однако, в качестве ключа шардирования рекомендуется брать значение хеш-функции от поля в таблице, которое позволит, с одной стороны, локализовать небольшие наборы данных на одном шарде, а с другой, обеспечит достаточно ровное распределение таких наборов по разным шардам в кластере. Например, идентификатор сессии (sess_id) пользователя позволит локализовать показы страниц одному пользователю на одном шарде, а с другой стороны сессии разных пользователей будут распределены равномерно по всем шардам в кластере. При условии, что значения поля sess_id будут иметь хорошее распределение. Ключ шардирования может быть нечисловой или составной, тогда можно применять втроенную хеширующую функцию [cityHash64](https://clickhouse.yandex/reference_ru.html#cityHash64). В данном режиме данные, записываемые на один из узлов кластера, по ключу шардирования будут перенаправляться на нужные шарды автоматически, правда, увеличивая трафик.
 
Более сложный способ заключается в том, чтобы снаружи ClickHouse вычислять нужный шард и выполнять запись напрямую в шардированную таблицу. Сложность здесь обусловлена тем, что нужно знать набор доступных узлов-шардов. Однако, запись становится более эффективной и механизм шардирования (определения нужного шарда) может быть более гибким.

### Репликация

ClickHouse поддерживает [репликацию](https://clickhouse.yandex/reference_ru.html#Репликация%20данных) данных, обеспечивая целостность данных на репликах. Для репликации данных используются специальные движки MergeTree-семейства:

* ReplicatedMergeTree
* ReplicatedCollapsingMergeTree
* ReplicatedAggregatingMergeTree
* ReplicatedSummingMergeTree

Репликация часто применяется с шардированием. Например, кластер из 6-узлов может содержать 3 шарда по 2 реплики. Следует отметить, что репликация не зависит от механизмов шардирования, и работает на уровне отдельных таблиц.
  
Запись данных может выполняться в любую из таблиц-реплик, ClickHouse выполняет автоматическую синхронизацию данных между всеми репликами.

### Примеры конфигурации ClickHouse-кластера

В качестве примеров будем рассматривать различные конфигурации для четырех узлов: ch63.smi2, ch64.smi2, ch65.smi2 и ch66.smi2.

Настройки выполняются в конфигурационном файле */etc/clickhouse-server/config.xml*.

#### Один шард и четыре реплики

![Один шард и четыре реплики](https://api.monosnap.com/rpc/file/download?id=BahALelyOJWu7ordZAFq6wvCaz6m3J)

```xml
<remote_servers>
    <!-- One shard, four replicas -->
    <one_shard_four_replicas>
        <!-- shard 01 --> 
        <shard>
            <!-- replica 01_01 -->
            <replica>
                <host>ch63.smi2</host>
            </replica>
            
            <!-- replica 01_02 -->
            <replica>
                <host>ch64.smi2</host>
            </replica>
            
            <!-- replica 01_03 -->
            <replica>
                <host>ch65.smi2</host>
            </replica>
            
            <!-- replica 01_04 -->
            <replica>
                <host>ch66.smi2</host>
            </replica>
        </shard>
    </one_shard_four_replicas>
</remote_servers>
```

Плюсы данной конфигурации:
* Наиболее надежный способ хранения данных.

Минусы:
* Для большинства задач будет хранениться избыточное количество копий данных.
* Поскольку в данной конфигурации только 1 шард, SELECT-запрос не может выполняться параллельно на разных узлах.
* Требуются дополнительные ресурсы на многократное реплицирование данных между всеми узлами.

#### Четыре шарда по одной реплике

![Четыре шарда по одной реплике](https://api.monosnap.com/rpc/file/download?id=X7lbGzFQ9HriQQ9QrlaLZRMPbQ4Sx1)

```xml
<remote_servers>
    <!-- Four shards, one replica -->
    <four_shards_one_replica>
        <!-- shard 01 --> 
        <shard>
            <!-- replica 01_01 -->
            <replica>
                <host>ch63.smi2</host>
            </replica>
        </shard>
        
        <!-- shard 02 --> 
        <shard>
            <!-- replica 02_01 -->
            <replica>
                <host>ch64.smi2</host>
            </replica>
        </shard>
        
        <!-- shard 03 --> 
        <shard>
            <!-- replica 03_01 -->
            <replica>
                <host>ch65.smi2</host>
            </replica>
        </shard>
        
        <!-- shard 04 --> 
        <shard>
            <!-- replica 04_01 -->
            <replica>
                <host>ch66.smi2</host>
            </replica>
        </shard>
    </four_shards_one_replica>
</remote_servers>
```

Плюсы данной конфигурации:
* Поскольку в данной конфигурации 4 шарда, SELECT-запрос может выполняться параллельно сразу на всех узлах кластера.

Минусы:
* Наименее надежный способ хранения данных (потеря узла - потеря порции данных).

#### Два шарда по две реплики

![Два шарда по две реплики](https://api.monosnap.com/rpc/file/download?id=HZblGQjLnOU6WlprWxb8W5FyixNlfY)

```xml
<remote_servers>
    <!-- Two shards, two replica -->
    <two_shards_two_replicas>
        <!-- shard 01 --> 
        <shard>
            <!-- replica 01_01 -->
            <replica>
                <host>ch63.smi2</host>
            </replica>
            
            <!-- replica 01_02 -->
            <replica>
                <host>ch64.smi2</host>
            </replica>
        </shard>
        
        <!-- shard 02 --> 
        <shard>
            <!-- replica 02_01 -->
            <replica>
                <host>ch65.smi2</host>
            </replica>
            
            <!-- replica 02_02 -->
            <replica>
                <host>ch66.smi2</host>
            </replica>
        </shard>
    </two_shards_two_replicas>
</remote_servers>
```

Данная конфигурация воплощает лучшие качества из первого и второго примеров:
* Поскольку в данной конфигурации 2 шарда, SELECT-запрос может выполняться параллельно на каждом из шардов в кластере.
* Относительно надежный способ хранения данных (потеря одного узла кластера не приводит к потере порции данных).

### Пример конфигурации кластеров в ansible

Конфигурация кластеров в [ansible](http://docs.ansible.com/ansible/index.html) может выглядеть следующим образом:

```xml
   - name: "one_shard_four_replicas"
     shards:
       - { name: "01", replicas: ["ch63.smi2", "ch64.smi2","ch65.smi2", "ch66.smi2"]}
   - name: "four_shards_one_replica"
     shards:
       - { name: "01", replicas: ["ch63.smi2"]}
       - { name: "02", replicas: ["ch64.smi2"]}
       - { name: "03", replicas: ["ch65.smi2"]}
       - { name: "04", replicas: ["ch66.smi2"]}
   - name: "two_shards_two_replicas"
     shards:
       - { name: "01", replicas: ["ch63.smi2", "ch64.smi2"]}
       - { name: "02", replicas: ["ch65.smi2", "ch66.smi2"]}
```

### Инструмент миграции DDL-запросов

На момент написания статьи ClickHouse имеет ряд особенностей (ограничений) связанных с DDL-запросами. [Цитата](https://clickhouse.yandex/reference_ru.html#Репликация%20данных):

> Реплицируются INSERT, ALTER (см. подробности в описании запроса ALTER). Реплицируются сжатые данные, а не тексты запросов. Запросы CREATE, DROP, ATTACH, DETACH, RENAME не реплицируются - то есть, относятся к одному серверу. Запрос CREATE TABLE создаёт новую реплицируемую таблицу на том сервере, где выполняется запрос; а если на других серверах такая таблица уже есть - добавляет новую реплику. Запрос DROP TABLE удаляет реплику, расположенную на том сервере, где выполняется запрос. Запрос RENAME переименовывает таблицу на одной из реплик - то есть, реплицируемые таблицы на разных репликах могут называться по разному.

Когда количество узлов кластера становится большим, то управление кластером становится неудобным. В результате мы создали простой и достаточно функциональный инструмент для миграции DDL-запросов в ClickHouse-кластер. 

# PHP-драйвер для работы с ClickHouse-кластером

В предыдущей [статье](https://habrahabr.ru/company/smi2/blog/314558/) мы уже рассказывали о нашем open-source [PHP-драйвере](https://github.com/smi2/phpClickHouse) для ClickHouse, а здесь кратко опишем его возможности по работе с кластером.

Для подключения к кластеру используется класс `ClickHouseDB\Cluster`:

```php
$cl = new ClickHouseDB\Cluster(
 ['host'=>'allclickhouse.smi2','port'=>'8123','username'=>'x','password'=>'x']
);

```

В DNS записи `allclickhouse.smi2` перечислены IP-адреса всех узлов: `ch63.smi2, ch64.smi2, ch65.smi2, ch66.smi2`.
