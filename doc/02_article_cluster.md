# Работа скластером ClickHouse из PHP, и выгрузка потоковых данных из Yandex Метрики 

*Это все - техническая заметка и наброски для статьи -нужно еще прорерайтить проорфографить - позже*

## О чем 

Статья состот из : 
- Работа в кластере 
- Работа с LogsApi Yam (? вынести вообще в отдельную стать может - в этой и так уже много заметок и LogsApi косячит на текущее время - данные не валидные / скрипты не проверить?)





### Вступление. Немного о масштабировании Clickhouse

Мы не будем глублоко рассказывать и отвечать на вопросы,  

* Что такое кластер ?
* Что такое реплики ?

предплоагая что читатель знаком с этим.... бла-бла


[Distributed](https://clickhouse.yandex/reference_ru.html#Distributed)

[Репликация данных](https://clickhouse.yandex/reference_ru.html#Репликация%20данных)


Сдесь будет очень коротко что такое кластер и репликация+шардирование и нет смысла переписывать документацию. 

( полный ревратй нужен )

Допустим имеется база данных, в которую осуществляется запись и чтение данных. 
В динамично растущих системах, объемы данных, как правило, быстро увеличиваются и рано или поздно можно столкнуться с проблемой, 
когда текущих ресурсов машины будет не хватать для нормальной работы.

Для решения этой проблемы применяют масштабирование. 

Масштабирование бывает 2-х видов — горизонтальное и вертикальное. 

*Вертикальное масштабирование* — наращивание мощностей одной машины — добавление CPU, RAM, HDD.
 
*Горизонтальное масштабирование* — добавление новых машин к существующим и распределение данных между ними.


Второй случай более сложен в конфигурации, но имеет ряд преимуществ: 
- Теоретически бесконечное масштабирование (машин можно поставить сколько угодно)
- Большая сохранность данных (только при использовании репликации) — машины могут располагаться в разных дата центрах (при падении одной из них, останутся другие)

В Yandex Metrica используется 300-500-600 серверов под ClickHouse. 




## Пример установки и конфигрурации Clickhouse

Для статьи и тестов мы сделаем, несколько конфиграций, надеюсь читатель не будет их использовать в продакшен системе, за исключением одной конфигруации. 
  
Когда у нас один ClickHouse сервер пусть называется `ch63.smi2`, все придельно просто, но мы захотели добавить несколько - для обеспеспечения максимальной безопастности. 

И так делаем : один сервер `ch63.smi2` и три копии данных `ch64.smi2 , ch65.smi2 , ch65.smi2`  - максимальная параноидальность -  и назовем такую конфигурацию = *repikator*

Тогда в конфигурационном CH файле опишим: 

```xml
<repikator>
    <shard>
        <replica>
            <host>ch63.smi2</host>
        </replica>
        <replica>
            <host>ch64.smi2</host>
        </replica>
        <replica>
            <host>ch65.smi2</host>
        </replica>
        <replica>
            <host>ch66.smi2</host>
        </replica>
    </shard>
</repikator>		
```

Послышались вопросы из зала:  
* Простоите что это, зачем этот XML  ? 
* И как с этим работать , Писать в файл при каждом добавлении сервера ? 

Это конфг XML для создания в CH таблицы которую можно будет реплицировать и которую можно размазать/шардоировать 

Из документации: 

*Движок Distributed* -  не хранит данные самостоятельно, а позволяет обрабатывать запросы распределённо, на нескольких серверах. Чтение автоматически распараллеливается. 

*Репликация данных - Движок Replicated*  Репликация никак не связана с шардированием. На каждом шарде репликация работает независимо.

Получается чтобы создать копию одной и той-же таблицы на всех наших 4х серверах нам нужно создать таблицу с префиксом `Replicated*` 
Т/е если у вас движек таблицы `MergeTree` то получается `ReplicatedMergeTree` с указанием `repikator`


```sql

CREATE TABLE тут пример 

```

Ок в этом примере мы достиги максимальной безопастности но данные занимают очень много места 


#### Конфигруция только из шардов 


Создадим конфигурацию `sharovara` состоящую только из шардов - без реплик - ну вдруг вы такой рисковый читатель и доверяете своим HDD 


```xml
<sharovara>
    <shard>
        <replica>
            <host>ch63.smi2</host>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>ch64.smi2</host>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>ch65.smi2</host>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>ch66.smi2</host>
        </replica>
    </shard>
</sharovara>
```

Создадим таблицу : 


```sql

CREATE TABLE table_shara тут пример 


```

Получается если мы пишем в таблицу `table_shara` то данные пишутся сразу на все сервера и размазываются равномерно ( веса серверов выходят за рамки статьи)



### Нормальная конфигурация 


И так давайте "без фанатизма" ->  создаем нормальную конфгируцию назовем ее `pulse`, дано 4е сервера сделаем одну компию данных и по ?один? шард


```xml

<pulse>
    <shard>
        <replica>
            <host>ch63.smi2</host>
        </replica>
        <replica>
            <host>ch64.smi2</host>
        </replica>
    </shard>
    <shard>
        <replica>
            <host>ch65.smi2</host>
        </replica>
        <replica>
            <host>ch66.smi2</host>
        </replica>
    </shard>
</pulse>

```


Получается : 
ch63 -> имеет копию данных на ch64
ch65 -> имеет копию данных на ch66
И данные равномерно записываются в пропорции 50 на 50 на ch63 и ch65 сервера.





#### Итог конфигурации 

Если описать все написанное выше в виде ansible конфига, для наглядности ( возможно мы выложим его OpenSource ) 

```xml
 ansible.cluster1.yml
    - name: "pulse"
      shards:
        - { name: "01", replicas: ["ch63.smi2", "ch64.smi2"]}
        - { name: "02", replicas: ["ch65.smi2", "ch66.smi2"]}
    - name: "sharovara"
      shards:
        - { name: "01", replicas: ["ch63.smi2"]}
        - { name: "02", replicas: ["ch64.smi2"]}
        - { name: "03", replicas: ["ch65.smi2"]}
        - { name: "04", replicas: ["ch66.smi2"]}
    - name: "repikator"
      shards:
        - { name: "01", replicas: ["ch63.smi2", "ch64.smi2","ch65.smi2", "ch66.smi2"]}

```


Не стоит использовать в продакшене кластер:
sharovara - это для теста и пример сделан в этом кластере нет репликации, т/е если вылетает одна из нод вытеряете 1/4 данных.

pulse - это две копии на две шарды
sharovara - это 4е шары
repikator - 4е реплики - максимальная надежность для параноии.




### Перешардирование CH кластера


Исходя из общения с разработчикамми и из документации - это боль и страдание) 
 
 




# Отправка запросов из PHP в кластер
( Давайте тут сунем отсыл к первой части - там рассписанно про PHPClickHouse драйвер наш )



И так вы поставли наш php драйвер, для подключения к кластеру используем отдельный класс `ClickHouseDB\Cluster`


```php
$cl = new ClickHouseDB\Cluster(
  ['host'=>'allclickhouse.smi2','port'=>'8123','username'=>'x','password'=>'x']
);
```

Где в DNS записи `allclickhouse.smi2` перечисленны все IP адреса всех серверов: `ch64.smi2 , ch65.smi2 , ch66.smi2 , ch63.smi2`

Тогда это позволят изспользую DNS_RoundRobing - обезопасить себя от выпадения одной из нод. 

При использовании кластера класса - драйвер подключается ко всем нодам из списка IP адрессов основоного имени, и отправляет паралельно на каждую ноду `ping` запрос.  
  
Установим максимальное время за которое можно подключиться ко всем нодам:
```php
$cl->setScanTimeOut(2.5); // 2500 ms
```



Проверяем что состояние рабочее,
```php
if (!$cl->isReplicasIsOk())
{
    throw new Exception('Replica state is bad , error='.$cl->getError());
}
```



[Как работает проверка](https://clickhouse.yandex/reference_ru.html#system.replicas) : 

* Установленно соединение со всеми сервера перечисленным в DNS записи
* Проверка таблицы system.replicas что всё хорошо
  * not is_readonly
  * not is_session_expired
  * not future_parts > 20
  * not parts_to_check > 10
  * not queue_size > 20
  * not inserts_in_queue > 10
  * not log_max_index - log_pointer > 10
  * not total_replicas < 2 ( зависит от использумого cluster )
  * active_replicas < total_replicas


future_parts:       количество кусков с данными, которые появятся в результате INSERT-ов или слияний, которых ещё предстоит сделать
parts_to_check:     количество кусков с данными в очереди на проверку Кусок помещается в очередь на проверку, если есть подозрение, что он может быть битым.
log_max_index:      максимальный номер записи в общем логе действий
log_pointer:        максимальный номер записи из общего лога действий, которую реплика скопировала в свою очередь для выполнения, плюс единица

Если log_pointer сильно меньше log_max_index, значит что-то не так.

total_replicas:     общее число известных реплик этой таблицы
active_replicas:    число реплик этой таблицы, имеющих сессию в ZK; то есть, число работающих реплик


В данном случии мы запрашиваем все столбцы, то таблица может работать медленно, так как на каждую строчку делается несколько чтений из ZK.
Если не запрашивать последние 4 столбца (log_max_index, log_pointer, total_replicas, active_replicas), то таблица работает быстро.

В драйвере ,не запрашивать последние 4 столбца , реализованно через : 

```php
$cl->setSoftCheck(true);
```




Получаем список всех cluster
```php
print_r($cl->getClusterList());
// result
//    [0] => pulse
//    [1] => repikator
//    [2] => sharovara
```

Узнаем список node(ip) и кол-во шардов и количество реплик

```php
foreach (['pulse','repikator','sharovara','repikator3x','sharovara3x'] as $name)
{
    print_r($cl->getClusterNodes($name));
    echo "> $name , count shard   = ".$cl->getClusterCountShard($name)." ; count replica = ".$cl->getClusterCountReplica($name)."\n";
}
//result:
// pulse , count shard = 2 ; count replica = 2
// repikator , count shard = 1 ; count replica = 4
// sharovara , count shard = 4 ; count replica = 1

```


Получаем список node по имени кластера, или из sharded таблиц:

```php

$nodes=$cl->getNodesByTable('sharovara.body_views_sharded');

$nodes=$cl->getClusterNodes('sharovara');

```
- `client->truncateTable('tableName')`
- `cluster->getMasterNodeForTable('dbName.tableName') // node have is_leader=1`
- `cluster->truncateTable('dbName.tableName')`


Пример получениея размера таблиц или всех таблиц через отправку запроса на каждую ноду:

```php
foreach ($nodes as $node)
{
    echo "$node > \n";
    print_r($cl->client($node)->tableSize('adpreview_body_views_sharded'));
    print_r($cl->client($node)->tablesSize());
}

// сокращенный вариант 
$cl->getSizeTable('dbName.tableName'); 
```

Список таблиц кластера :
```php
$cl->getTables()
```


### Определение лидера в кластере 

```php

$cl->getMasterNodeForTable('dbName.tableName') // node have is_leader=1

```

Это позволяет понять какая нода является лидером, т/е у нее утановлен признак `is_leader=1` для отправки на нее запроса, на удаление данных или для изменения структуры.

Для очисти всех данных в одной таблицы во всем кластере 

```php 

$cl->truncateTable('dbName.tableName')`

```


## Миграции в кластере и примеры


Мы используем [mybatis](http://www.mybatis.org/migrations/) для нашей MySQL базы данных. 

Есть много других красивых продуктов для реализации миграции. 



Стоте а зачем эти миграции нужны и что это вообще такое: 

* [Версионная миграция структуры базы данных: основные подходы](https://habrahabr.ru/post/121265/)
* [Простые миграции с PHPixie Migrate](https://habrahabr.ru/post/315254/)
* [Управление скриптами миграции или MyBatis Scheme Migration Extended](https://habrahabr.ru/post/129290/)



Вопросы читателю: 

* Допустим у вас таблица из 50 колонок , а елси их 200 или 500 ?
* Вы ведете отдельную документацию по каждой колонке ?
* Вы помните когда каждую колонку добавил ?
* Вы согласовываете alter table запросы внутри комманды ? 




Я ( мы ) начали реализовывать проект позволяющий осуществлять миграции в CH,  [phpMigrationsClickhouse](https://github.com/smi2/phpMigrationsClickhouse)
проект находится в зачаточном состоянии и после последнего митапа - когда CH возмет на себя раскатывания изменений по всем нодам -> проект остановился.

Данная статья содержит опрос - пожалуйта проголосуйте

* Убери с глаз ваш велосипед на php 
* Стоит подождать реализации в самом CH и продолжить реализацию 
* Мне интересно решение - т/к мы делаем сами костыльные решения для этого.




### phpMigrationsClickhouse


Отправляем запрос на сервера выбранного кластера в виде миграции,
Если хоть на одном происходит ошибка, выполняем откат запросов

Перед выполнение миграции, каждый узел кластера еще раз проверяется на доступность через `ping()`.


```php

$mclq=new ClickHouseDB\Cluster\Migration($cluster_name);
$mclq->addSqlUpdate('CREATE DATABASE IF NOT EXISTS cluster_tests');
$mclq->addSqlDowngrade('DROP DATABASE IF EXISTS shara');


if (!$cl->sendMigration($mclq))
{
    throw new Exception('sendMigration error='.$cl->getError());
}

```


""" ДОПИСАТЬ С ПРИМЕРОМ из readme.md """


# Выгружайте сырые данные из Метрики через Logs API


(в отдельную статью?)